{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMOSingleLayerCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Pl2DYN1aGurdpdVusJp_EfedCTtqoqBA",
      "authorship_tag": "ABX9TyMEKzt8zFbZljjRHjZi3V9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remanx2558/MIDBReviews/blob/master/ELMOSingleLayerCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0OuXUEEeYlX",
        "colab_type": "code",
        "outputId": "bba517d4-663e-4cfc-e8fa-276bcbb6afc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        " \n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax87hdRseYst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        " \n",
        "# load the document\n",
        "filename = '/content/drive/My Drive/malia/train.csv'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "#print(tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtofTc3Skw34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# printing the list using loop \n",
        "for x in range(1000): \n",
        "    print(tokens[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC1zKzhdiWgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_trian):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i-RosO1eYvY",
        "colab_type": "code",
        "outputId": "d1302910-5e69-46c4-8091-62647806931b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "negative_docs = process_docs('txt_sentoken/neg', True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-403fdc0bab5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-054f782421bb>\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(directory, is_trian)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# walk through all files in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0;31m# skip any reviews in the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_trian\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv9'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'listdir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jWxAHRaeYyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKGmXWmonDxk",
        "colab_type": "code",
        "outputId": "b60fc7ad-9870-4495-b7ac-ad747fd0d7da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "movie_reviews = pd.read_csv(r\"/content/drive/My Drive/malia/train.csv\")\n",
        "\n",
        "movie_reviews.isnull().values.any()\n",
        "movie_reviews.sentiment= movie_reviews.sentiment.fillna(0.0).astype(int)#this will conver float into int and also manage missing values\n",
        "\n",
        "movie_reviews.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25005, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTK774DSnD0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkJoFDgsnD52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8naFNoynD4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = []\n",
        "sentences = list(movie_reviews['text'])\n",
        "for sen in sentences:\n",
        "    reviews.append(preprocess_text(sen))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afzqz6rTnDvH",
        "colab_type": "code",
        "outputId": "216868dc-fd41-4653-fd2f-9659950db1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(movie_reviews.columns.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['text' 'sentiment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7cv6HVReY1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainX, testX, trainy, testY = train_test_split(movie_reviews.text, movie_reviews.sentiment, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNV4Sucgo0RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "\tdump(dataset, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZp7BCxooOK2",
        "colab_type": "code",
        "outputId": "5a0b9674-0f79-4fc7-e2b4-b24777852eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "save_dataset([trainX,trainy], 'train.pkl')\n",
        "save_dataset([testX,testY], 'test.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_B29FLDocOP",
        "colab_type": "code",
        "outputId": "1447561d-0e82-4e97-ca18-77612c9fd0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        " \n",
        "\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "trainLines, trainLabels = load_dataset('train.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRX6JJPAocVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B46lA3W2ocXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLZUWH2ocbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0muRs_aCqN24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsVRbRkdq_JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        " \n",
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        " \n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwKYprvgrF_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPTgRDgdrJ-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H02s_V_RrLkc",
        "colab_type": "text"
      },
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTdAaFWRrKDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoZVBrMYrKJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EysxokfcrKG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        " \n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9eCY1QSrKB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "\t# channel 1\n",
        "\tinputs1 = Input(shape=(length,))\n",
        "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(length,))\n",
        "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(length,))\n",
        "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\n",
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\treturn model\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Xysq4Lr6j7",
        "colab_type": "code",
        "outputId": "bf600e45-36ef-4416-c873-654e2075e077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "print(trainX.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 2459\n",
            "Vocabulary size: 73658\n",
            "(16753, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBt0aRw3sBNv",
        "colab_type": "code",
        "outputId": "33f107e4-deee-496e-b15e-3a9f51552ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "# fit model\n",
        "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 2459)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 2459)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 2459)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 2459, 100)    7365800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 2459, 100)    7365800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 2459, 100)    7365800     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 2456, 32)     12832       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 2454, 32)     19232       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 2452, 32)     25632       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 2456, 32)     0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 2454, 32)     0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 2452, 32)     0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 1228, 32)     0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 1227, 32)     0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 1226, 32)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 39296)        0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 39264)        0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 39232)        0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 117792)       0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           1177930     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,333,037\n",
            "Trainable params: 23,333,037\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "16753/16753 [==============================] - 29s 2ms/step - loss: 0.3963 - accuracy: 0.8041\n",
            "Epoch 2/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.1267 - accuracy: 0.9558\n",
            "Epoch 3/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0283 - accuracy: 0.9926\n",
            "Epoch 4/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0074 - accuracy: 0.9981\n",
            "Epoch 5/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0012 - accuracy: 0.9999\n",
            "Epoch 6/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0274 - accuracy: 0.9918\n",
            "Epoch 7/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0087 - accuracy: 0.9974\n",
            "Epoch 8/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0064 - accuracy: 0.9974\n",
            "Epoch 9/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0047 - accuracy: 0.9981\n",
            "Epoch 10/10\n",
            "16753/16753 [==============================] - 23s 1ms/step - loss: 0.0104 - accuracy: 0.9971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FSNdR79sC9Z",
        "colab_type": "code",
        "outputId": "dd398d7a-3fbc-4d8b-acb7-53d037a6a332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        " \n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "print(trainX.shape, testX.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 2459\n",
            "Vocabulary size: 73658\n",
            "(16753, 2459) (8252, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLWT5jn0sDLw",
        "colab_type": "code",
        "outputId": "8b15069b-7906-457d-8843-bcd8365631cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        " \n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        " \n",
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        " \n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "print(trainX.shape, testX.shape)\n",
        " \n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        " \n",
        "# evaluate model on training dataset\n",
        "loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        " \n",
        "# evaluate model on test dataset dataset\n",
        "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 2459\n",
            "Vocabulary size: 73658\n",
            "(16753, 2459) (8252, 2459)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 99.677670\n",
            "Test Accuracy: 86.912262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afUvMq38sDPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funuT1FztzoS",
        "colab_type": "code",
        "outputId": "b72f7062-c193-4b19-ebea-847f65ef079c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-133693f30906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best movie ever\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeA5m2m8tzry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stage 5: Evaluation\n",
        "import tensorflow as tf\n",
        " \n",
        "def get_prediction(sentence):\n",
        "    tokens = create_tokenizer(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = model(\"as fuck\", training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: negative.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: positive.\".format(\n",
        "            output))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7p0m0BrySHZ",
        "colab_type": "code",
        "outputId": "a5e3dde4-d537-43cf-81f2-4accd8d62400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "get_prediction()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-fac7c9a8fd3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_prediction() missing 1 required positional argument: 'sentence'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RiIXjCGySLb",
        "colab_type": "code",
        "outputId": "8edf77cf-2376-4c80-c99c-1a977e11ac69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6434295038235344, 0.8691226243972778]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEuBBBFstzu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}